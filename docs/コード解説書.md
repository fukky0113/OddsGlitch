# OddsGlitch コード解説書

netkeiba「競馬新聞」ページからレース基本情報・出走馬・過去5走・オッズを抽出し、規定のJSON形式で出力する Python スクリプトのコード解説です。

---

## 1. 全体構成

```
OddsGlitch/
├── main.py           # CLI エントリポイント
├── config.py         # URL・ヘッダ・エンコーディング等の設定
├── schemas.py        # 出力JSONの型定義 (dataclass)
├── fetcher.py        # HTML 取得・オッズ API 取得
├── builder.py        # パーサ結果とオッズの統合・RaceResult 組み立て
├── parser/
│   ├── __init__.py
│   ├── utils.py      # テキスト正規化・ID抽出
│   ├── race_info.py  # レース基本情報の抽出
│   └── horses.py     # 出走馬・過去5走の抽出
└── docs/
    └── コード解説書.md  # 本ドキュメント
```

**データの流れ**

1. `main.py` が `race_id` を受け取り、HTML と（オプションで）オッズ API を取得
2. `fetcher.py` が shutuba_past.html を取得し BeautifulSoup を返す
3. `parser/race_info.py` と `parser/horses.py` が HTML からレース情報・馬情報を抽出
4. `fetcher.fetch_odds()` で単勝オッズ・人気を API から取得
5. `builder.py` がパーサ結果とオッズを統合し `RaceResult` を組み立て
6. `schemas.RaceResult.to_json()` で JSON 文字列を出力

---

## 2. 各モジュールの解説

### 2.1 config.py — 設定

| 定数 | 説明 |
|------|------|
| `BASE_URL` | 実際に取得するページ（shutuba_past.html）。過去5走テーブルがここにのみ存在するため。 |
| `NEWSPAPER_URL` | 出力 JSON の `source_url` に記載する URL（newspaper.html）。 |
| `ODDS_API_URL` | 単勝オッズ・人気を取得する netkeiba 内部 API の URL。 |
| `HEADERS` | リクエスト時の User-Agent 等。ブラウザに近いヘッダで取得。 |
| `PAGE_ENCODING` | ページの文字コード。netkeiba は **EUC-JP**。 |
| `REQUEST_TIMEOUT` | リクエストのタイムアウト秒数。 |
| `REQUEST_INTERVAL` | （将来用）連続リクエスト間の待機秒数。 |

---

### 2.2 schemas.py — 出力型定義

dataclass で出力 JSON の構造を定義しています。`asdict()` で辞書化し、`json.dumps(..., ensure_ascii=False)` で JSON にしています。

#### PastRace（過去走1レース）

| フィールド | 型 | 説明 |
|------------|-----|------|
| `run` | int | 何走前か（1=前走, 2=2走前, …）。 |
| `date` | str \| None | 実施日（例: "2025.11.24"）。 |
| `venue` | str \| None | 競馬場名（東京・京都など）。 |
| `position` | int \| None | 着順。 |
| `time` | str \| None | 走破タイム（例: "1:46.0"）。 |
| `last_3f` | str \| None | 上がり3ハロン（例: "32.7"）。 |
| `popularity` | int \| None | そのレース時点の人気（X人気）。 |

#### Horse（出走馬1頭）

| フィールド | 型 | 説明 |
|------------|-----|------|
| `number` | int | 馬番。 |
| `horse_id` | str | netkeiba の馬 ID（10桁）。 |
| `horse_name` | str | 馬名。 |
| `jockey` | str \| None | 騎手名。 |
| `weight` | float \| None | 斤量。 |
| `odds` | float \| None | **単勝オッズ**（現在レース）。API から取得。 |
| `popularity` | int \| None | **人気順位**（現在レース）。API から取得。 |
| `past_races` | list[PastRace] | 過去走のリスト（最大5）。 |

#### RaceBasicInfo（レース基本情報）

レース名・発走時刻・芝/ダ・距離・馬場・天候・日付・場所など。`race_info_text` は RaceData01/02 の生テキストを結合したもの。

#### RaceInfo

設計書の `race_info` 用。`lap_prediction` と `development` は空オブジェクト `{}` で固定。

#### RaceResult（ルート）

`source_url`, `race_id`, `race`, `race_info`, `horses`, `poplar` を持つ。`to_dict()` / `to_json(indent)` でシリアライズ。

---

### 2.3 fetcher.py — HTML・オッズの取得

#### URL ビルダー

- **build_url(race_id)**  
  shutuba_past.html の URL を組み立てる（取得元）。
- **build_source_url(race_id)**  
  newspaper.html の URL を組み立てる（出力の `source_url` 用）。

#### HTML 取得

- **fetch_html(race_id)**  
  - `requests.get()` で shutuba_past.html を取得。  
  - `response.content` を **EUC-JP** として BeautifulSoup に渡し、`from_encoding=config.PAGE_ENCODING` で正しくデコード。  
  - 返り値は `BeautifulSoup`（lxml パーサ）。

- **fetch_html_from_file(filepath)**  
  ローカル HTML をバイナリで読み、同様に EUC-JP としてパース。テスト・開発用。

#### オッズ API

- **fetch_odds(race_id)**  
  - `api_get_jra_odds.html?race_id=...&type=1` に GET。  
  - `type=1` は単勝。  
  - レスポンス例: `{"status":"result","data":{"odds":{"1":{"01":["39.6","","10"], ...}}}`  
    - `"1"` が単勝。  
    - `"01"` が馬番（2桁ゼロパッド）。  
    - 配列は `[オッズ, 空, 人気順位]`。  
  - `status != "result"` のとき（オッズ未公開・レース前など）は `None` を返し、stderr にメッセージを出す。  
  - 成功時は `{"01": {"odds": 39.6, "popularity": 10}, ...}` 形式の辞書を返す。

---

### 2.4 parser/utils.py — 共通ユーティリティ

- **normalize_text(text)**  
  - NFKC 正規化。  
  - `\xa0`（nbsp）・全角スペースを半角スペースに。  
  - 連続空白を1つに。  
  - 前後トリム。  
  HTML 由来のテキストをパースしやすくするために全パーサで使用。

- **extract_int(text)** / **extract_float(text)**  
  文字列から最初の整数／浮動小数を正規表現で抽出。

- **extract_horse_id(href)**  
  `https://db.netkeiba.com/horse/2023106850` のような URL から 10 桁の馬 ID を取得。  
  `re.search(r"/horse/(\d{10})", href)` でマッチ。

---

### 2.5 parser/race_info.py — レース基本情報の抽出

対象は **1つの** `div.RaceList_NameBox` のみ。その配下だけを解析します。

#### 処理の流れ

1. **parse_race_info(soup)**  
   - `soup.select_one("div.RaceList_NameBox")` で name_box を取得。  
   - 無ければ空の `RaceBasicInfo()` を返す。

2. **レース名**  
   - `h1.RaceName` の直下テキストを取得（アイコンは除く）。  
   - `_detect_grade()` で `Icon_GradeType1/2/3` を G1/G2/G3 に変換し、`"きさらぎ賞 (G3)"` のように付加。

3. **race_info_text**  
   - `div.RaceData01` と `div.RaceData02` のテキストを正規化し、`" / "` で結合。  
   - 発走時刻・芝/ダ・距離・馬場・天候・回次・場所・日目などが含まれる生テキスト。

4. **個別項目**（いずれも RaceData01 のテキストを正規表現で解析）  
   - **post_time**: `(\d{1,2}:\d{2})`  
   - **course_type**: 「芝」「ダ」の含まれる方  
   - **distance**: `(\d{3,4})m` の数値  
   - **track_condition**: `馬場:(\S+)`  
   - **weather**: `天候:(\S+)`  

5. **race_date**  
   - ページ全体の `<title>` から `YYYY年MM月DD日` を正規表現で取得し、`YYYY/MM/DD` に変換。

6. **venue**  
   - RaceData02 の `span` を順に取り、2番目を場所名とする（例: ["2回", "京都", "4日目", ...] → "京都"）。

---

### 2.6 parser/horses.py — 出走馬・過去5走の抽出

対象は **1つの** `table.Shutuba_Past5_Table`（id="sort_table" を優先）のみ。

#### テーブル構造の前提

- 各行は `tr.HorseList`。  
- 列: 枠番（Waku1 等）→ **馬番（td.Waku）** → マーク → **馬情報（td.Horse_Info）** → **騎手・斤量（td.Jockey）** → 過去走（td.Past）が複数 → 休み・血統等（td.Rest）。

#### 馬1行の解析 _parse_horse_row(tr)

1. **馬番**  
   - `tr.find_all("td", class_="Waku")` の**先頭**が馬番（枠番は Waku1, Waku2 なので class が完全一致 "Waku" の td のみ）。

2. **馬名・horse_id**  
   - `td.Horse_Info` 内の `div.Horse02 > a`。  
   - テキストが馬名、`href` から `extract_horse_id()` で馬 ID。

3. **騎手・斤量**  
   - `td.Jockey` 内の `a` のテキストが騎手名。  
   - class が "Barei" でない `span` のうち、float にできる最初の値を斤量とする。

4. **過去走**  
   - `tr.find_all("td", class_="Past")` を順に `_parse_past_cell(td, run_index)` に渡す。  
   - run_index は 1, 2, 3, …（1走前、2走前、…）。

#### 過去走1セル _parse_past_cell(td, run_index)

各 `td.Past` 内は `div.Data_Item` で、その中に:

- **Data01**: 日付・場所（最初の span）、着順（span.Num）
- **Data03**: 「12頭 8番 5人 マーカンド 56.0」形式 → 正規表現 `(\d+)人` で**人気**を取得
- **Data05**: 芝/ダ・距離・走破タイム → 正規表現 `(\d:\d{2}\.\d)` で**タイム**
- **Data06**: 通過・上がり3F・馬体重 → 最初の `(\d{2}\.\d)` を **last_3f** として取得

これらを `PastRace` に詰めて返します。Data_Item が無いセルは `None`。

#### parse_horses(soup)

- 上記テーブルを取得し、`tr.HorseList` をループ。  
- 各行を `_parse_horse_row` し、馬名が取れたものだけリストに追加。  
- 最後に馬番でソートして返す。

---

### 2.7 builder.py — 統合と RaceResult 組み立て

- **_merge_odds(horses, odds_data)**  
  - `odds_data` は `fetch_odds()` の返り値（馬番 "01", "02", ... をキーとする辞書）。  
  - 各 `Horse` の `number` を 2 桁ゼロパッドしたキーで `odds_data` を引き、`horse.odds` と `horse.popularity` を**in-place**で設定。

- **build_race_result(soup, race_id, odds_data=None)**  
  - `parse_race_info(soup)` でレース基本情報。  
  - `parse_horses(soup)` で馬リスト。  
  - `_merge_odds(horses, odds_data)` でオッズ・人気をマージ。  
  - `RaceResult(source_url=build_source_url(race_id), race_id=..., race=..., race_info=RaceInfo(), horses=..., poplar=[])` を返す。

---

### 2.8 main.py — CLI エントリポイント

- **parse_args()**  
  - 引数: `race_id`（必須）、`-o/--output`、`--local`、`--no-odds`、`--indent`。  
  - `--local` を付けると HTML はローカルファイルから読み、オッズは取得しない（`--no-odds` は別途指定可能）。  
  - `--no-odds` でオッズ API を呼ばない。

- **main()**  
  1. HTML 取得（`--local` なら `fetch_html_from_file`、否则 `fetch_html`）。  
  2. `--no-odds` でなければ `fetch_odds(race_id)` を実行。  
  3. `build_race_result(soup, race_id, odds_data)` で `RaceResult` を生成。  
  4. `to_json(indent=args.indent)` で JSON 文字列にし、`--output` があればファイルに書き、なければ stdout に出力。  
  5. レース名・頭数・日付を stderr にサマリー表示。

ログ・メッセージはすべて stderr に出し、stdout は JSON のみにすることで、パイプやリダイレクトで扱いやすくしています。

---

## 3. データソースと設計上の注意

| 項目 | 説明 |
|------|------|
| 取得元 HTML | **shutuba_past.html**。newspaper.html には過去5走テーブルが無いため、データ取得はこちら。 |
| 出力の source_url | 設計書に合わせ **newspaper.html** の URL を記載。 |
| エンコーディング | ページは **EUC-JP**。BeautifulSoup に `from_encoding="euc-jp"` を渡してパース。 |
| オッズ | 単勝のみ。API が未公開・レース前の場合は `odds` / `popularity` は null。 |
| 過去走の人気 | HTML の Data03「X人」から抽出。オッズ API は使わない。 |

---

## 4. 拡張・変更時のポイント

- **セレクタ変更**  
  netkeiba の HTML クラス名が変わった場合は、`parser/race_info.py` の `RaceList_NameBox` / `RaceData01` / `RaceData02`、`parser/horses.py` の `Shutuba_Past5_Table` / `Horse_Info` / `Jockey` / `Data_Item` 等のセレクタを合わせて修正。

- **オッズの種類を増やす**  
  `config.ODDS_API_URL` の `type` や、API レスポンスの `odds["2"]`（複勝）等を解析し、`schemas.Horse` にフィールドを追加して `fetcher.fetch_odds()` と `builder._merge_odds()` を拡張する。

- **負荷対策**  
  複数 race_id を連続取得する場合は、`config.REQUEST_INTERVAL` を使ってリクエスト間に sleep を入れることを推奨。

以上が、本プロジェクトのコード解説です。
